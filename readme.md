# AIM825 Course Project Submission

## Project Title
**Multimodal Visual Question Answering with Amazon Berkeley Objects Dataset**

## Submitted by
- **Abhishek Kumar Singh** (MT2024006)
- **Naval Kishore Singh Bisht** (MT2024099)

---

## Project Overview

This project explores multimodal visual question answering (VQA) using the Amazon Berkeley Objects (ABO) dataset. The aim is to develop a system that can answer questions about images by leveraging both visual and textual information.

## Instructions

- Please refer to the `VRReport.pdf` for detailed methodology, experiments, and results.
- For inference, you need to download the script and the trained model from the link below.
- As model size is big we decided to submit it saperately.
- The codes to merge dataset is in Concatenate folder

## Important Links

- **Inference Script & Model:** [Download Here](https://drive.google.com/drive/folders/1SvCJb3pOXY1Nn0QQblSQNosytMyYbDtI?usp=drive_link)

---

## Notes

- Ensure you have all dependencies installed as mentioned in the report and the requirements.txt.
- After downloading the code from google drive. unzip it and copy your own data into its data folder to run model inference on it. read the instrcuctions.txt. We have worked on Kaggle and local conda environment.

---